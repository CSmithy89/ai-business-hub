<?xml version="1.0" encoding="UTF-8"?>
<!--
  Story Context: DM-11.4 - Parallel MCP Server Connections
  Generated: 2025-01-01

  This context file provides implementation guidance for adding parallel
  MCP server connections to the HYVVE AgentOS platform.
-->
<story-context>
  <metadata>
    <story-id>DM-11.4</story-id>
    <epic>DM-11 - Advanced Features &amp; Optimizations</epic>
    <title>Parallel MCP Server Connections</title>
    <points>3</points>
    <priority>Medium</priority>
  </metadata>

  <summary>
    Add parallel connection capability to the MCP client to reduce agent startup
    time. Currently, MCP servers connect sequentially, with each connection waiting
    for the previous one to complete. This story implements asyncio.gather-based
    parallel connections with timeout handling and graceful degradation.
  </summary>

  <!-- =========================================================================
       EXISTING MCP IMPLEMENTATION
       ========================================================================= -->

  <existing-code>
    <file path="agents/mcp/client.py" description="MCP Client - Core implementation">
      <purpose>
        MCPClient manages connections to multiple MCP servers. Currently, the
        connect() method handles one server at a time. The caller (create_mcp_bridge)
        iterates sequentially through enabled servers.
      </purpose>

      <key-class name="MCPConnection">
        <description>
          Manages a single MCP server subprocess connection. Handles:
          - Subprocess lifecycle (start/stop)
          - JSON-RPC 2.0 communication via stdin/stdout
          - Request locking to prevent response mixing
        </description>
        <methods>
          <method name="start">Launches subprocess, waits 0.3s for init</method>
          <method name="stop">Terminates process gracefully, kills on timeout</method>
          <method name="list_tools">Sends tools/list JSON-RPC request</method>
          <method name="call_tool">Sends tools/call JSON-RPC request</method>
        </methods>
      </key-class>

      <key-class name="MCPClient">
        <description>
          Client for managing multiple MCP server connections. Provides:
          - Connection pooling (_connections dict)
          - Tool caching (_tools_cache dict)
          - Unified tool invocation interface
        </description>
        <current-connect-method>
          <![CDATA[
async def connect(self, server_name: str) -> bool:
    """
    Connect to an MCP server.

    Starts the server process, discovers available tools, and caches them.
    Returns True if connection successful, False otherwise.
    """
    # Already connected
    if server_name in self._connections:
        return True

    # Check if server exists and is enabled
    server_config = self.config.servers.get(server_name)
    if not server_config or not server_config.enabled:
        return False

    try:
        connection = MCPConnection(server_config)
        await connection.start()
        self._connections[server_name] = connection

        # Cache available tools
        tools = await connection.list_tools()
        for tool in tools:
            tool["_server"] = server_name
        self._tools_cache[server_name] = tools

        return True

    except MCPConnectionError as e:
        logger.error(f"Failed to connect to MCP server '{server_name}': {e}")
        return False
    except MCPProtocolError as e:
        # Started but tool discovery failed - clean up
        if server_name in self._connections:
            await self._connections[server_name].stop()
            del self._connections[server_name]
        return False
          ]]>
        </current-connect-method>

        <attributes>
          <attribute name="config" type="MCPConfig">Server configurations</attribute>
          <attribute name="_connections" type="Dict[str, MCPConnection]">Active connections by name</attribute>
          <attribute name="_tools_cache" type="Dict[str, List[Dict]]">Cached tools per server</attribute>
        </attributes>
      </key-class>
    </file>

    <file path="agents/mcp/config.py" description="MCP Configuration">
      <purpose>
        Defines MCPServerConfig and MCPConfig models. Contains default server
        configurations and environment variable resolution.
      </purpose>

      <key-classes>
        <class name="MCPServerConfig">
          <attributes>
            <attr name="name">Unique server identifier</attr>
            <attr name="command">Command to launch (e.g., "uvx")</attr>
            <attr name="args">Command arguments</attr>
            <attr name="env">Environment variables (supports ${VAR} pattern)</attr>
            <attr name="enabled">Whether server should be connected</attr>
          </attributes>
        </class>

        <class name="MCPConfig">
          <attributes>
            <attr name="servers">Dict of server name to MCPServerConfig</attr>
            <attr name="default_timeout">Default request timeout (30s)</attr>
            <attr name="max_retries">Maximum retry attempts (3)</attr>
          </attributes>
        </class>
      </key-classes>

      <default-servers>
        <![CDATA[
DEFAULT_MCP_SERVERS: Dict[str, MCPServerConfig] = {
    "github": MCPServerConfig(
        name="github",
        command="uvx",
        args=["mcp-server-github"],
        env={"GITHUB_TOKEN": "${GITHUB_TOKEN}"},
        description="GitHub repository access and management",
        enabled=True,
    ),
    "brave": MCPServerConfig(
        name="brave",
        command="uvx",
        args=["mcp-server-brave-search"],
        env={"BRAVE_API_KEY": "${BRAVE_API_KEY}"},
        description="Brave web search",
        enabled=True,
    ),
    "filesystem": MCPServerConfig(
        name="filesystem",
        command="uvx",
        args=["mcp-server-filesystem", "--allowed-directories", "/tmp/hyvve"],
        env={},
        description="Local filesystem access (sandboxed to /tmp/hyvve)",
        enabled=True,
    ),
}
        ]]>
      </default-servers>
    </file>

    <file path="agents/mcp/a2a_bridge.py" description="MCP Tool Bridge">
      <purpose>
        Bridges MCP tools to agent-compatible format. Contains the create_mcp_bridge
        factory function that currently connects to servers sequentially.
      </purpose>

      <current-sequential-connection>
        <![CDATA[
async def create_mcp_bridge(
    config: Optional[MCPConfig] = None,
    connect_enabled: bool = True,
) -> MCPToolBridge:
    """
    Create and initialize an MCP tool bridge.

    Factory function that creates an MCPClient, optionally connects to
    enabled servers, and returns a configured MCPToolBridge.
    """
    if config is None:
        config = get_default_mcp_config()

    client = MCPClient(config)

    if connect_enabled:
        # PROBLEM: Sequential connection - slow
        for server_name, server_config in config.servers.items():
            if server_config.enabled:
                success = await client.connect(server_name)
                if not success:
                    logger.warning(
                        f"Failed to connect to MCP server '{server_name}'. "
                        f"Tools from this server will not be available."
                    )

    bridge = MCPToolBridge(client)
    return bridge
        ]]>
      </current-sequential-connection>
    </file>

    <file path="agents/mcp/__init__.py" description="MCP Module Exports">
      <exports>
        <export>MCPServerConfig</export>
        <export>MCPConfig</export>
        <export>MCPClient</export>
        <export>MCPConnection</export>
        <export>MCPConnectionError</export>
        <export>MCPProtocolError</export>
        <export>MCPToolBridge</export>
        <export>create_mcp_bridge</export>
        <export>get_default_mcp_config</export>
      </exports>
      <note>
        After implementation, also export:
        - ConnectionResult (new dataclass)
      </note>
    </file>
  </existing-code>

  <!-- =========================================================================
       PARALLEL EXECUTION PATTERNS IN CODEBASE
       ========================================================================= -->

  <patterns>
    <pattern name="Parallel A2A Calls" file="agents/a2a/client.py">
      <description>
        The A2A client implements parallel agent calls using asyncio.gather.
        This pattern should be followed for parallel MCP connections.
      </description>
      <code>
        <![CDATA[
async def call_agents_parallel(
    self,
    calls: List[Dict[str, Any]],
    caller_id: str = "dashboard_gateway",
) -> Dict[str, A2ATaskResult]:
    """
    Call multiple agents in parallel.

    Executes multiple A2A calls concurrently using asyncio.gather,
    which is more efficient than sequential calls.
    """
    if not calls:
        return {}

    # Build parallel tasks
    tasks = []
    agent_ids = []

    for call in calls:
        agent_id = call.get("agent_id")
        if not agent_id:
            continue
        agent_ids.append(agent_id)
        tasks.append(self.call_agent(agent_id=agent_id, ...))

    if not tasks:
        return {}

    # Execute all calls in parallel
    # Using return_exceptions=True ensures all calls complete even if some fail
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # Build result dictionary, converting exceptions to error results
    output: Dict[str, A2ATaskResult] = {}
    for agent_id, result in zip(agent_ids, results):
        if isinstance(result, A2ATaskResult):
            output[agent_id] = result
        elif isinstance(result, Exception):
            output[agent_id] = A2ATaskResult(success=False, error=str(result), ...)

    return output
        ]]>
      </code>
      <key-points>
        <point>Use return_exceptions=True to prevent one failure from canceling others</point>
        <point>Collect results with zip() to match task to result</point>
        <point>Handle both success and exception cases in result processing</point>
      </key-points>
    </pattern>

    <pattern name="Health Check Pattern" file="agents/mesh/discovery.py">
      <description>
        The discovery service implements health_check_all for external agents.
        Similar pattern can be used for MCP server health reporting.
      </description>
      <code>
        <![CDATA[
async def health_check_all(self) -> Dict[str, AgentHealth]:
    """
    Check health of all external agents.

    Returns:
        Dict mapping agent names to their health status
    """
    registry = get_registry()
    external_agents = registry.list_external()

    results: Dict[str, AgentHealth] = {}
    for agent in external_agents:
        health = await self.check_agent_health(agent.name)
        results[agent.name] = health

    return results
        ]]>
      </code>
    </pattern>
  </patterns>

  <!-- =========================================================================
       STARTUP SEQUENCE INTEGRATION
       ========================================================================= -->

  <startup-integration>
    <file path="agents/main.py">
      <description>
        FastAPI application entry point. Uses @app.on_event("startup") for
        initialization. MCP connections are NOT currently initialized at startup
        (they are created on-demand when create_mcp_bridge is called).
      </description>

      <current-startup-event>
        <![CDATA[
@app.on_event("startup")
async def startup_event():
    """Initialize services and register agents on startup."""
    logger.info("AgentOS starting up...")

    # OpenTelemetry tracing
    otel_settings = get_otel_settings()
    if otel_settings.otel_enabled:
        configure_tracing()
        instrument_app(app)

    # Register teams in the A2A registry
    for team_name, config in TEAM_CONFIG.items():
        team = config["factory"](session_id="registry", user_id="system")
        registry.register_team(team, override_id=team_name)

    # Validate CCR connection if enabled
    if settings.ccr_enabled:
        ccr_ok = await validate_ccr_connection()

    # Initialize Dashboard Gateway Agent
    await startup_dashboard_gateway()

    # Initialize PM Agent A2A interfaces
    await startup_pm_agents_a2a()
        ]]>
      </current-startup-event>

      <recommended-mcp-integration>
        <![CDATA[
# Add after PM agent initialization in startup_event():

# Initialize MCP connections in parallel (DM-11.4)
await startup_mcp_connections()

# New function to add:
async def startup_mcp_connections():
    """Initialize all MCP server connections in parallel."""
    global _mcp_client

    config = get_default_mcp_config()
    _mcp_client = MCPClient(config)

    logger.info("Starting parallel MCP server connections...")
    start_time = time.time()

    # Connect all enabled servers in parallel
    results = await _mcp_client.connect_all()

    elapsed = time.time() - start_time
    logger.info(f"MCP connection phase completed in {elapsed:.2f}s")

    # Identify failed connections for background retry
    failed = [name for name, result in results.items() if not result.success]

    if failed:
        logger.warning(f"Failed to connect to MCP servers: {failed}")
        # Schedule background retries (non-blocking)
        asyncio.create_task(retry_failed_mcp_connections(failed))
        ]]>
      </recommended-mcp-integration>
    </file>
  </startup-integration>

  <!-- =========================================================================
       HEALTH ENDPOINT PATTERN
       ========================================================================= -->

  <health-endpoint>
    <current-health-endpoint file="agents/main.py" lines="1048-1063">
      <![CDATA[
@app.get("/health")
async def health(request: Request):
    """Health check endpoint."""
    return {
        "status": "ok",
        "version": "0.2.0",
        "registered_agents": len(registry.list_cards()),
        "environment": {
            "database_configured": bool(settings.database_url),
            "redis_configured": bool(settings.redis_url),
        },
        "tenant_context": {
            "user_id": getattr(request.state, "user_id", None),
            "workspace_id": getattr(request.state, "workspace_id", None)
        }
    }
      ]]>
    </current-health-endpoint>

    <recommended-extension>
      <![CDATA[
# Add MCP connection status to health endpoint:
@app.get("/health")
async def health(request: Request):
    """Health check endpoint."""
    # Get MCP connection status
    mcp_status = {}
    if _mcp_client:
        health_info = _mcp_client.get_connection_health()
        connected, total = _mcp_client.get_healthy_server_count()
        mcp_status = {
            "connected": connected,
            "total": total,
            "servers": health_info,
        }

    return {
        "status": "ok",
        "version": "0.2.0",
        "registered_agents": len(registry.list_cards()),
        "environment": {
            "database_configured": bool(settings.database_url),
            "redis_configured": bool(settings.redis_url),
        },
        "mcp": mcp_status,  # NEW
        "tenant_context": {...}
    }
      ]]>
    </recommended-extension>
  </health-endpoint>

  <!-- =========================================================================
       EXISTING TEST PATTERNS
       ========================================================================= -->

  <test-patterns>
    <file path="agents/mcp/__tests__/test_client.py">
      <description>
        Existing unit tests for MCPClient using mocks. Follow these patterns
        for the new parallel connection tests.
      </description>

      <pattern name="Mock MCPConnection">
        <![CDATA[
@pytest.mark.asyncio
async def test_connect_starts_connection(self, mcp_config):
    """Should start connection and cache tools."""
    client = MCPClient(mcp_config)

    with patch.object(MCPConnection, "start", new_callable=AsyncMock):
        with patch.object(
            MCPConnection,
            "list_tools",
            new_callable=AsyncMock,
            return_value=[{"name": "tool1", "description": "Test tool"}],
        ):
            result = await client.connect("test")

    assert result is True
    assert "test" in client._connections
    assert "test" in client._tools_cache
        ]]>
      </pattern>

      <pattern name="Test Connection Failure">
        <![CDATA[
@pytest.mark.asyncio
async def test_connect_returns_false_on_connection_error(self, mcp_config):
    """Should return False if connection fails."""
    client = MCPClient(mcp_config)

    with patch.object(
        MCPConnection,
        "start",
        new_callable=AsyncMock,
        side_effect=MCPConnectionError("Failed"),
    ):
        result = await client.connect("test")

    assert result is False
    assert "test" not in client._connections
        ]]>
      </pattern>

      <pattern name="Config Fixture">
        <![CDATA[
@pytest.fixture
def mcp_config(self):
    """Create a test MCP config."""
    return MCPConfig(
        servers={
            "test": MCPServerConfig(
                name="test",
                command="echo",
                args=["test"],
                enabled=True,
            ),
            "disabled": MCPServerConfig(
                name="disabled",
                command="echo",
                args=["disabled"],
                enabled=False,
            ),
        }
    )
        ]]>
      </pattern>
    </file>
  </test-patterns>

  <!-- =========================================================================
       IMPLEMENTATION GUIDANCE
       ========================================================================= -->

  <implementation-guidance>
    <step number="1" title="Add ConnectionResult Dataclass">
      <location>agents/mcp/client.py (top of file, after imports)</location>
      <description>
        Create a dataclass to represent the result of a connection attempt.
        This provides structured information for logging and health reporting.
      </description>
      <code>
        <![CDATA[
from dataclasses import dataclass

@dataclass
class ConnectionResult:
    """Result of a single MCP server connection attempt."""
    server_name: str
    success: bool
    tools_count: int = 0
    error: Optional[str] = None
    retry_scheduled: bool = False
        ]]>
      </code>
    </step>

    <step number="2" title="Add connect_all Method to MCPClient">
      <location>agents/mcp/client.py (MCPClient class)</location>
      <description>
        Add the parallel connection method that uses asyncio.gather with
        per-connection timeouts.
      </description>
      <key-considerations>
        <item>Use asyncio.wait_for for per-connection timeout</item>
        <item>Use asyncio.gather with return_exceptions=True</item>
        <item>Handle both timeout and other exceptions</item>
        <item>Return Dict[str, ConnectionResult] for detailed status</item>
      </key-considerations>
    </step>

    <step number="3" title="Add Health Status Methods">
      <location>agents/mcp/client.py (MCPClient class)</location>
      <description>
        Add methods to report connection health status for the health endpoint.
      </description>
      <methods>
        <method name="get_connection_health">
          Returns Dict[str, bool] mapping server names to connected status
        </method>
        <method name="get_healthy_server_count">
          Returns tuple (connected_count, total_count)
        </method>
      </methods>
    </step>

    <step number="4" title="Add retry_failed_connections Method">
      <location>agents/mcp/client.py (MCPClient class)</location>
      <description>
        Add background retry logic with exponential backoff for failed connections.
      </description>
      <key-considerations>
        <item>Use exponential backoff (2^attempt seconds)</item>
        <item>Limit to max_retries (default 3)</item>
        <item>Log each retry attempt</item>
        <item>Return final ConnectionResult for each server</item>
      </key-considerations>
    </step>

    <step number="5" title="Update Module Exports">
      <location>agents/mcp/__init__.py</location>
      <description>Export the new ConnectionResult dataclass.</description>
    </step>

    <step number="6" title="Update Startup Integration (Optional)">
      <location>agents/main.py</location>
      <description>
        Optionally add MCP initialization to startup_event if you want
        connections established at startup rather than on-demand.
        Also update the /health endpoint to include MCP status.
      </description>
      <note>
        This step may be deferred if MCP connections should remain on-demand.
        Discuss with team whether startup initialization is desired.
      </note>
    </step>

    <step number="7" title="Write Unit Tests">
      <location>agents/mcp/__tests__/test_parallel_connections.py (new file)</location>
      <description>
        Create comprehensive tests for the new parallel connection functionality.
      </description>
      <test-cases>
        <test>test_connect_all_parallel_execution - verify all servers connect concurrently</test>
        <test>test_connect_all_timeout_handling - verify individual timeouts work</test>
        <test>test_connect_all_partial_failure - verify one failure doesn't block others</test>
        <test>test_connect_all_empty_servers - verify empty list returns empty results</test>
        <test>test_get_connection_health - verify accurate health status</test>
        <test>test_get_healthy_server_count - verify correct counts</test>
        <test>test_retry_failed_connections_backoff - verify exponential backoff timing</test>
        <test>test_retry_failed_connections_max_retries - verify retry limit</test>
      </test-cases>
    </step>
  </implementation-guidance>

  <!-- =========================================================================
       THREAD SAFETY NOTES
       ========================================================================= -->

  <thread-safety>
    <note>
      The MCPClient class uses internal dictionaries (_connections, _tools_cache)
      that are modified during connection. Since all operations are async and run
      on a single event loop, no additional locking is needed for the dictionaries
      themselves. The existing MCPConnection._request_lock handles JSON-RPC
      serialization.
    </note>
    <consideration>
      When implementing connect_all(), ensure that each connection task operates
      on its own MCPConnection instance. The parallel tasks will be writing to
      shared _connections and _tools_cache dicts, but since asyncio is
      cooperative and we await each gather result before processing, there are
      no race conditions.
    </consideration>
  </thread-safety>

  <!-- =========================================================================
       PERFORMANCE EXPECTATIONS
       ========================================================================= -->

  <performance>
    <metric name="Sequential (current)">~500ms per server</metric>
    <metric name="Parallel (target)">~500ms total regardless of server count</metric>

    <expected-improvement>
      <servers count="3">3x faster (1500ms -> 500ms)</servers>
      <servers count="5">5x faster (2500ms -> 500ms)</servers>
      <servers count="10">10x faster (5000ms -> 500ms)</servers>
    </expected-improvement>

    <notes>
      The improvement scales linearly with server count since all connections
      happen concurrently. The actual startup time is bounded by the slowest
      server connection (or timeout if specified).
    </notes>
  </performance>

  <!-- =========================================================================
       DEPENDENCIES
       ========================================================================= -->

  <dependencies>
    <dependency story="DM-06" status="complete">
      MCP Integration - Provides base MCPClient and MCPConnection classes
    </dependency>
    <dependency story="DM-08" status="complete">
      Quality Hardening - Caching and rate limiting patterns
    </dependency>
    <dependency story="DM-09" status="complete">
      Observability - OpenTelemetry tracing for connection metrics
    </dependency>
  </dependencies>

  <!-- =========================================================================
       FILES TO CREATE
       ========================================================================= -->

  <files-to-create>
    <file path="agents/mcp/__tests__/test_parallel_connections.py">
      Unit tests for parallel connection logic, health status, and retry
    </file>
  </files-to-create>

  <!-- =========================================================================
       FILES TO MODIFY
       ========================================================================= -->

  <files-to-modify>
    <file path="agents/mcp/client.py">
      <changes>
        <change>Add ConnectionResult dataclass</change>
        <change>Add connect_all() method with parallel execution</change>
        <change>Add get_connection_health() method</change>
        <change>Add get_healthy_server_count() method</change>
        <change>Add retry_failed_connections() method</change>
      </changes>
    </file>
    <file path="agents/mcp/__init__.py">
      <changes>
        <change>Export ConnectionResult</change>
      </changes>
    </file>
    <file path="agents/main.py" optional="true">
      <changes>
        <change>Add startup_mcp_connections() function</change>
        <change>Update startup_event() to call MCP initialization</change>
        <change>Add MCP status to /health endpoint</change>
      </changes>
    </file>
  </files-to-modify>

  <!-- =========================================================================
       ACCEPTANCE CRITERIA CHECKLIST
       ========================================================================= -->

  <acceptance-criteria>
    <criterion id="AC1" description="MCP connections happen in parallel">
      All enabled servers connect concurrently via asyncio.gather()
    </criterion>
    <criterion id="AC2" description="Startup time reduced by ~Nx">
      Measured improvement matches expected parallelization
    </criterion>
    <criterion id="AC3" description="Individual failures don't block others">
      One server timing out doesn't prevent other servers from connecting
    </criterion>
    <criterion id="AC4" description="Failed connections logged with retry">
      Failed connections are logged with error details and scheduled for retry
    </criterion>
    <criterion id="AC5" description="Health check reflects partial connectivity">
      Health endpoint shows which servers are connected vs disconnected
    </criterion>
  </acceptance-criteria>
</story-context>
